{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import time\n",
    "import numpy as np\n",
    "from torchvision.utils import draw_bounding_boxes \n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface.extendedmodels import Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_model = Emotion.loadModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.28 ðŸš€ Python-3.9.16 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 3050 Ti Laptop GPU, 3902MiB)\n",
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "results = model(\"videoplayback.mp4\", stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {0: \"angry\", 1: \"disgust\", 2:\"fear\", 3:\"happy\", 4:\"sad\", 5:\"surprise\", 6:\"neutral\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def draw(image, bounding_boxes, frame_width, frame_height, id=1):\n",
    "    dh, dw, _ = image.shape\n",
    "    for x, y,w,h in bounding_boxes:\n",
    "\n",
    "        # Draw a rectangle on the image\n",
    "        x = int(x) \n",
    "        y = int(y) \n",
    "        w = int(w) \n",
    "        h = int(h)\n",
    "\n",
    "        l = int((x - w / 2))\n",
    "        r = int((x + w / 2))\n",
    "        t = int((y - h / 2))\n",
    "        b = int((y + h / 2))\n",
    "\n",
    "        image = cv2.rectangle(image, (l, t), (r, b), (0, 0, 255), 2)        \n",
    "        # img = cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)  # (0, 255, 0) is the color in BGR format\n",
    "        cropped_image = image[t:b, l:r]\n",
    "        cropped_image = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2GRAY)\n",
    "        cropped_image = cv.resize(cropped_image, (48,48))\n",
    "        result = senti_model.predict(np.expand_dims(cropped_image, axis=0), verbose=False)\n",
    "        emotion = np.argmax(result)\n",
    "        image = cv2.putText(image, labels[emotion], (l, t - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "    return image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model(\"videoplayback.mp4\", stream=True)import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Open the video file\n",
    "video_path = \"/home/sawaiz/Documents/Lab/In Progress/Video face detection/notebooks/videoplayback.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output1.avi',cv2.VideoWriter_fourcc('M','J','P','G'), fps, (frame_width,frame_height))\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Run YOLOv8 tracking on the frame, persisting tracks between frames\n",
    "        results = model(frame, verbose=False, classes=[0], conf= 0.4)\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = draw(frame, results[0].boxes.xywh, frame_width, frame_height, id=1)\n",
    "        \n",
    "        out.write(annotated_frame)\n",
    "\n",
    "        # Display the annotated frame\n",
    "        # cv2.imshow(\"YOLOv8 Tracking\", annotated_frame)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release() # Release the video file\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#42 seconds for a two minute video\n",
    "#54 second for a resized. So no need to resize\n",
    "#just cropping takes 43 seconds\n",
    "#Takes really long with sentiment analyzer of deepface ofc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(srcimg,input_height, input_width, keep_ratio=True):\n",
    "    if keep_ratio and srcimg.shape[0] != srcimg.shape[1]:\n",
    "        hw_scale = srcimg.shape[0] / srcimg.shape[1]\n",
    "        if hw_scale > 1:\n",
    "            newh, neww = input_height, int(input_width / hw_scale)\n",
    "            img = cv2.resize(srcimg, (neww, newh), interpolation=cv2.INTER_AREA)\n",
    "        else:\n",
    "            newh, neww = int(input_height * hw_scale), input_width\n",
    "            img = cv2.resize(srcimg, (neww, newh), interpolation=cv2.INTER_AREA)\n",
    "    else:\n",
    "        img = cv2.resize(srcimg, (input_width, input_height), interpolation=cv2.INTER_AREA)\n",
    "    return img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the model, Elapsed time: 20.2766056060791 seconds\n"
     ]
    }
   ],
   "source": [
    "video_file_path = '/home/sawaiz/Documents/Lab/In Progress/Video face detection/experimentation_videos/videoplayback.mp4'  # Replace with the path to your video file\n",
    "cap = cv.VideoCapture(video_file_path)\n",
    "fps = cap.get(cv.CAP_PROP_FPS)\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "fourcc = cv.VideoWriter_fourcc(*'XVID')\n",
    "out = cv.VideoWriter('output1.avi',cv.VideoWriter_fourcc('M','J','P','G'), fps/2, (frame_width,frame_height))\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video file.\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = resize_image(frame, frame_height, frame_width)\n",
    "\n",
    "\n",
    "    # cv.imshow(\"Img\", frame)\n",
    "\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "out.release() # Release the video file\n",
    "# cv.destroyAllWindows()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"For the model, Elapsed time: {elapsed_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61d516de8b14c17ac4afc753bbb958bb470f4ac68b1779d85b7bbffb57d87d9f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.16 ('object')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
