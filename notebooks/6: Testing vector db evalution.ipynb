{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from deepface import DeepFace\n",
    "import cv2\n",
    "from deepface.extendedmodels import Emotion\n",
    "import numpy as np\n",
    "import os\n",
    "import pinecone\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish pinecone vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to pinecone and create an index for saving facial embeddings of the database if \n",
    "#it doesn't exist\n",
    "pinecone.init(api_key = \"c3be1f15-9d16-4c29-954b-82a30326a330\", environment=\"us-west4-gcp-free\")\n",
    "\n",
    "existing_indexes = pinecone.list_indexes() \n",
    " \n",
    "if \"verification\" not in existing_indexes:\n",
    "    pinecone.create_index(name = \"verification\", metric = \"euclidean\", dimension=128)\n",
    " \n",
    "index = pinecone.Index(\"verification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and processing all the images into embeddings and save to pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = []\n",
    "main_path = \"/home/sawaiz/Documents/Lab/In Progress/Video face detection/database\"\n",
    "for folder in os.listdir(main_path):\n",
    "    for file in os.listdir(os.path.join(main_path, folder)):\n",
    "        img_paths.append(os.path.join(main_path, folder, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:00<00:00, 420787.78it/s]\n"
     ]
    }
   ],
   "source": [
    "ids = []\n",
    "for i in tqdm.tqdm(range(0, len(img_paths))):\n",
    "    ids.append(img_paths[i].split(\"/\")[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:09<00:00,  3.29it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings = []\n",
    "for i in tqdm.tqdm(range(0, len(img_paths))):\n",
    "    img_path = img_paths[i]\n",
    "    embedding = DeepFace.represent(img_path = img_path, model_name=\"Facenet\", enforce_detection=False, detector_backend = \"skip\")[0][\"embedding\"]\n",
    "    embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(ids, columns = [\"ids\"])\n",
    "df[\"embedding\"] = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 31}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.upsert(vectors=zip(df.ids, df.embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_model = Emotion.loadModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {0: \"angry\", 1: \"disgust\", 2:\"fear\", 3:\"happy\", 4:\"sad\", 5:\"surprise\", 6:\"neutral\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "colors = dict()\n",
    "for i in range(1,17):\n",
    "    colors[\"id \" + str(i)] = (randint(0,255), randint(0,255), randint(0,255))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions for processing sentiment and identifying faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(cropped_image):    \n",
    "    cropped_image = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2GRAY)\n",
    "    cropped_image = cv2.resize(cropped_image, (48,48))\n",
    "    result = senti_model.predict(np.expand_dims(cropped_image, axis=0), verbose=False)\n",
    "    emotion = np.argmax(result)\n",
    "    return emotion\n",
    "\n",
    "def face_verification(cropped_image):\n",
    "    target_embedding = DeepFace.represent(img_path = img_path, model_name=\"OpenFace\", enforce_detection=False, detector_backend = \"skip\")[0][\"embedding\"]\n",
    "    results = index.query(queries=[target_embedding], top_k = 1)\n",
    "    id = results[\"results\"][0][\"matches\"][0][\"id\"]\n",
    "    return id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(image, bounding_boxes, id=1):\n",
    "    for x, y,w,h in bounding_boxes:\n",
    "\n",
    "        # Draw a rectangle on the image\n",
    "        x = int(x) \n",
    "        y = int(y) \n",
    "        w = int(w) \n",
    "        h = int(h)\n",
    "\n",
    "        l = int((x - w / 2))\n",
    "        r = int((x + w / 2))\n",
    "        t = int((y - h / 2))\n",
    "        b = int((y + h / 2))\n",
    "    \n",
    "        cropped_image = image[t:b, l:r]\n",
    "        emotion = sentiment_analysis(cropped_image)\n",
    "        id = face_verification(cropped_image)\n",
    "\n",
    "        image = cv2.rectangle(image, (l, t), (r, b), colors[\"id 1\"], 2)   \n",
    "        image = cv2.putText(image, f\"1:{labels[emotion]}\", (l, t - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, colors[\"id 1\"], 2)\n",
    "\n",
    "    return image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Open the video file\n",
    "video_path = \"experimentation_videos/videoplayback.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('model_output/output1.avi',cv2.VideoWriter_fourcc('M','J','P','G'), fps, (frame_width,frame_height))\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Run YOLOv8 tracking on the frame, persisting tracks between frames\n",
    "        results = model(frame, classes=[0], verbose=False, conf=0.4)\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = draw(frame, results[0].boxes.xywh, id=1)\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"YOLOv8 Tracking\", annotated_frame)\n",
    "        # out.write(annotated_frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61d516de8b14c17ac4afc753bbb958bb470f4ac68b1779d85b7bbffb57d87d9f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.16 ('object')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
