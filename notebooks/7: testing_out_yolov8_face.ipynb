{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.yolov8Face.main import YOLOv8_face\n",
    "labels = {0: \"angry\", 1: \"disgust\", 2:\"fear\", 3:\"happy\", 4:\"sad\", 5:\"surprise\", 6:\"neutral\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface.extendedmodels import Emotion\n",
    "senti_model = Emotion.loadModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: senti_base/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: senti_base/assets\n"
     ]
    }
   ],
   "source": [
    "senti_model.save(\"senti_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "def draw(image, boxes, scores, id=1):\n",
    "    for box, score in zip(boxes, scores):\n",
    "        x, y, w, h = box.astype(int)\n",
    "        # Draw rectangle\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 0, 255), thickness=2)\n",
    "\n",
    "        cropped_image = image[y:y+h, x:x+w]\n",
    "        cropped_image = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2GRAY)\n",
    "        cropped_image = cv2.resize(cropped_image, (48,48))\n",
    "        result = senti_model.predict(np.expand_dims(cropped_image, axis=0), verbose=False)\n",
    "        emotion = np.argmax(result)\n",
    "        image = cv2.putText(image, f\"{labels[emotion]}\", (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), thickness=1)\n",
    "\n",
    "\n",
    "    return image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOLOv8_face_detector = YOLOv8_face('models/yolov8Face/weights/yolov8n-face.onnx', conf_thres=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the video file\n",
    "video_path = \"experimentation_videos/videoplayback.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('model_output/output1.avi',cv2.VideoWriter_fourcc('M','J','P','G'), fps, (frame_width,frame_height))\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Detect Objects\n",
    "        boxes, scores, classids, kpts = YOLOv8_face_detector.detect(frame)\n",
    "        # annotated_frame = draw(frame, boxes, frame_width, frame_height, id=1)\n",
    "        # Draw detections\n",
    "        dstimg = YOLOv8_face_detector.draw_detections(frame, boxes, scores, kpts)\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"YOLOv8 Tracking\", dstimg)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release() # Release the video file\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "61d516de8b14c17ac4afc753bbb958bb470f4ac68b1779d85b7bbffb57d87d9f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.16 ('object')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
